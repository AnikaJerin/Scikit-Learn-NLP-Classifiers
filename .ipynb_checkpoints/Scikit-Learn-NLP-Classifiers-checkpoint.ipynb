{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load_file And Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sentiment:\n",
    "    NEGATIVE = \"NEGATIVE\"\n",
    "    NEUTRAL = \"NEUTRAL\"\n",
    "    POSITIVE = \"POSITIVE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Love the book, great story line, keeps you entertained.for a first novel from this author she did a great job,  Would definitely recommend!',\n",
       " 4.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name='./data/sentiment/Books_small.json'\n",
    "reviews=[]\n",
    "with open(file_name) as  f:\n",
    "    for line in f:\n",
    "        review=json.loads(line)\n",
    "#         print(review['reviewText'])\n",
    "#         print(review['overall'])\n",
    "#         break #cz right now you don't have to read all reviews; just reading one review and breaking the loop\n",
    "        reviews.append((review['reviewText'],review['overall']))\n",
    "reviews[5] #taking reviews with score\n",
    "# reviews[5][0] #taking only review \n",
    "# reviews[5][1] #taking only scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Same thing Using Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    def __init__(self,text,score):\n",
    "        self.text=text\n",
    "        self.Score=score\n",
    "        self.sentiment=self.get_sentiment()\n",
    "    def get_sentiment(self):\n",
    "        if self.Score <= 2:\n",
    "            return \"NEGATIVE\"\n",
    "        elif self.Score == 3:\n",
    "            return \"NUTRAL\"\n",
    "        else:\n",
    "            return \"POSITIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love the book, great story line, keeps you entertained.for a first novel from this author she did a great job,  Would definitely recommend!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name='./data/sentiment/Books_small.json'\n",
    "reviews=[]\n",
    "with open(file_name) as  f:\n",
    "    for line in f:\n",
    "        review=json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'],review['overall'])) #creating objects for Review class\n",
    "# reviews[5].text\n",
    "reviews[5].sentiment\n",
    "reviews[5].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Texts into Quantitive Vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of words : if we have two lines such as \"this book was so good\" and \"this book was very bad\". First we have to break this into a\n",
    "#### dicionary of tokens; may be a unigram like a single word as a dictionary. here--> this, book, was, so, good,very , bad\n",
    "#### to convert this into vector all we have to do is map ones and zeroses to the terms over here \n",
    "#### so, this book was so good would have ones mapped to the 1st 5 words but last 2 words would have zero mapped into it  cz it has negative vibe. \n",
    "#### For the 1st line it would be 1,1,1,1,1,0,0\n",
    "#### For the 2nd line it would be 1,1,1,0,0,1,1 (see the ss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "'''If you do not use a random_state in train_test_split, every time you make the split you might get a different set of train and test data points '''\n",
    "training, test=train_test_split(reviews, test_size=0.33, random_state=42) # 33% of the total data are gonna be in the training set\n",
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "Da Silva takes the divine by storm with this unique new novel.  She develops a world unlike any others while keeping it firmly in the real world.  This is a very well written and entertaining novel.  I was quite impressed and intrigued by the way that this solid storyline was developed, bringing the readers right into the world of the story.  I was engaged throughout and definitely enjoyed my time spent reading it.I loved the character development in this novel.  Da Silva creates a cast of high school students who actually act like high school students.  I really appreciated the fact that none of them were thrown into situations far beyond their years, nor did they deal with events as if they had decades of life experience under their belts.  It was very refreshing and added to the realism and impact of the novel.  The friendships between the characters in this novel were also truly touching.Overall, this novel was fantastic.  I can&#8217;t wait to read more and to find out what happens next in the series.  I&#8217;d definitely recommend this debut novel by Da Silva to those who want a little YA fun with a completely unique & shocking storyline.Please note that I received a complimentary copy of this work in exchange for an honest review.\n"
     ]
    }
   ],
   "source": [
    "## passing the training_set into bag of words vectorizer\n",
    "print(training[4].sentiment)\n",
    "print(training[4].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x=[x.text for x in training] # storing the text part of the \n",
    "train_y=[x.sentiment for x in training] # storing the sentiment part of the either pos or neg\n",
    "\n",
    "test_x=[x.text for x in test] # storing the text part of the;we'll vectorize them and use this to predict our data and compares them to the test_y\n",
    "test_y=[x.sentiment for x in test] # it contains the actual label (pos or neg) for test_x\n",
    "train_x[0]\n",
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# This book was so good !\n",
    "# This book was very bad\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "print(train_x[0])\n",
    "# print(train_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7086)\t1\n",
      "  (0, 1148)\t1\n",
      "  (0, 350)\t2\n",
      "  (0, 1800)\t1\n",
      "  (0, 6595)\t1\n",
      "  (0, 562)\t1\n",
      "  (0, 3054)\t1\n",
      "  (0, 1558)\t1\n",
      "  (0, 6475)\t1\n",
      "  (0, 6593)\t1\n",
      "  (0, 2895)\t1\n",
      "  (0, 7353)\t1\n",
      "  (0, 539)\t1\n",
      "  (0, 1515)\t1\n",
      "  (0, 5197)\t1\n",
      "  (0, 3545)\t1\n",
      "  (0, 2007)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x_vectors[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifcation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Every new Myke Cole book is better than the last, and this is no exception. If you haven't read the Shadow Ops series before start with Control Point, but go ahead and order Fortress Frontier and Breach Zone as well - you're going to want them.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "clf_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "test_x[0] # Just to check \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_dec.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_vectors_toarray=train_x_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(train_x_vectors_toarray, train_y)\n",
    "\n",
    "clf_gnb.predict(test_x_vectors[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unisoft/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_log.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "### Look at the entire test set and see how accurately we predict every one of these test examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8242424242424242\n",
      "0.7575757575757576\n",
      "0.8121212121212121\n",
      "0.8303030303030303\n"
     ]
    }
   ],
   "source": [
    "# Mean Accuracy\n",
    "print(clf_svm.score(test_x_vectors, test_y)) # pass in test_x_vectors and see how this prediction for this compares to our actual test_y\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_y))\n",
    "print(clf_log.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91319444, 0.22222222])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Precision and recall are two extremely important model evaluation metrics. \n",
    "While precision refers to the percentage of your results which are relevant, \n",
    "recall refers to the percentage of total relevant results correctly classified by your algorithm.\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)'''\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "# we have 91% percent for positive and 22% for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
